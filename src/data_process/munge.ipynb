{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import lxml.html\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "import nltk\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as w\n",
    "MIN_SENT_LENGTH = 10\n",
    "ENTRY_NUMBER = 5000\n",
    "random_state = 114514"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction functions\n",
    "def sentence_length(text):\n",
    "    text = str(text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    numberofsentences = len(sentences)\n",
    "    total_words = 0\n",
    "    for i in sentences:\n",
    "        total_words += len(i.split())\n",
    "    avg_sentence = total_words / numberofsentences\n",
    "    return numberofsentences, avg_sentence\n",
    "\n",
    "def repetitivewords(text):\n",
    "    text = str(text)\n",
    "    token = nltk.word_tokenize(text.lower())\n",
    "    synsets = []\n",
    "    for i in token:\n",
    "        synsets.extend(w.synsets(i))\n",
    "    synonyms = []\n",
    "    for synset in synsets:\n",
    "        synonyms.append([lemma.name() for lemma in synset.lemmas()])\n",
    "    repeat = 0\n",
    "    for index in range(len(synonyms)):\n",
    "        for nextindex in range(index+1, len(synonyms)):\n",
    "            if len(set(synonyms[index]) & set(synonyms[nextindex])) > 0:\n",
    "                repeat += 1\n",
    "    return repeat / len(token)\n",
    "\n",
    "def entropy(text):\n",
    "    text = str(text)\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokennumber = Counter(tokens)\n",
    "    total = len(tokens)\n",
    "    numberofprobs = []\n",
    "    for count in tokennumber.values():\n",
    "        prob = count / total\n",
    "        numberofprobs.append(prob)\n",
    "    entropy = 0.0\n",
    "    for i in numberofprobs:\n",
    "        if i > 0:\n",
    "            entropy -= i * (math.log(i, 2))\n",
    "    return entropy\n",
    "\n",
    "def avg_punctuation():\n",
    "    text = str(text)\n",
    "    num_sent = len(nltk.sent_tokenize(text))\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    punc_count = 0\n",
    "    # what about the ' in there's or I'am ? or quotation mark? or numbered lists?\n",
    "    for t in tokens:\n",
    "        punc_count += 1 if t in string.punctuation else 0\n",
    "    return punc_count / num_sent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>One of the film’s chief strengths is its perfo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The two crashes that claimed the lives of 346 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>In conclusion, the recent video showing the se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>With such overwhelming devastation, it can be ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3254</th>\n",
       "      <td>In the 1970s, Kirstein turned his attention to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5117</th>\n",
       "      <td>The escalation of tensions between the US and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5543</th>\n",
       "      <td>The use of language in a court of law is cruci...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3657</th>\n",
       "      <td>Among the nominees for Best Original Screenpla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>Iran has been a hot topic in the news lately, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Another factor that can drive up the cost of a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  generated\n",
       "1316  One of the film’s chief strengths is its perfo...          1\n",
       "9     The two crashes that claimed the lives of 346 ...          1\n",
       "1462  In conclusion, the recent video showing the se...          1\n",
       "3289  With such overwhelming devastation, it can be ...          1\n",
       "3254  In the 1970s, Kirstein turned his attention to...          1\n",
       "...                                                 ...        ...\n",
       "5117  The escalation of tensions between the US and ...          1\n",
       "5543  The use of language in a court of law is cruci...          1\n",
       "3657  Among the nominees for Best Original Screenpla...          1\n",
       "784   Iran has been a hot topic in the news lately, ...          1\n",
       "145   Another factor that can drive up the cost of a...          1\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def munge_turbo(files, set_features = False):\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        lines = open(file).readlines()\n",
    "        text_list = [l.strip() for l in lines]\n",
    "        text_list = list(filter(lambda t: t!= \"\" and len(t.split()) >= MIN_SENT_LENGTH, text_list))\n",
    "        df = pd.DataFrame({'text': text_list,\n",
    "                       'generated': 1})\n",
    "        if set_features:\n",
    "            df['sent_length'], df['avg_sent_length'] = zip(*df['text'].apply(sentence_length))\n",
    "            df['repetitive_words'] = df['text'].apply(repetitivewords)\n",
    "            df['text_entropy'] = df['text'].apply(entropy)\n",
    "        dfs.append(df)\n",
    "    gpt = pd.concat(dfs, ignore_index=True)\n",
    "    if ENTRY_NUMBER:\n",
    "        gpt = gpt.sample(ENTRY_NUMBER, random_state=random_state)\n",
    "    gpt.to_csv('turbo.csv', index=False)\n",
    "    return gpt\n",
    "munge_turbo(glob.glob('../turbo_generator/data/*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def munge_gpt3(files):\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        lines = open(file).readlines()\n",
    "        text_list = [l.strip() for l in lines]\n",
    "        text_list = list(filter(lambda t: t!= \"\" and len(t.split()) >= 15, text_list))\n",
    "        df = pd.DataFrame({\n",
    "                    'text': text_list,\n",
    "                       'generated': 1,\n",
    "                    #    'sent_length':[],\n",
    "                    #    'avg_sent_length':[],\n",
    "                    #    'repetitive_words':[],\n",
    "                    #    'text_entropy':[],\n",
    "                    #    'avg_punctuation' :[]\n",
    "                       })\n",
    "        # df['sent_length'], df['avg_sent_length'] = zip(*df['text'].apply(sentence_length))\n",
    "        # df['repetitive_words'] = df['text'].apply(repetitivewords)\n",
    "        # df['text_entropy'] = df['text'].apply(entropy)\n",
    "        # dfs.append(df)\n",
    "\n",
    "    gpt = pd.concat(dfs, ignore_index=True)\n",
    "    if ENTRY_NUMBER:\n",
    "        gpt = gpt.sample(ENTRY_NUMBER, random_state=random_state)\n",
    "    gpt.to_csv('gpt3.csv', index=False)\n",
    "    return gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def munge_gpt(keyword, chunk = True):\n",
    "    dfs = []\n",
    "    files = glob.glob(f'gpt_data/{keyword}*.csv')\n",
    "    for file in files:\n",
    "        raw_df = pd.read_csv(file)\n",
    "        text_list = [str(t).strip() for t in raw_df['text']]\n",
    "        if chunk:\n",
    "            text_list = [t.split('\\n') for t in text_list]\n",
    "            text_list = list(itertools.chain(*text_list))\n",
    "            text_list = list(filter(lambda t: t!= \"\" and len(t.split()) >= MIN_SENT_LENGTH, text_list))\n",
    "        df = pd.DataFrame({'text': text_list,\n",
    "                       'generated': 1})\n",
    "        dfs.append(df)\n",
    "\n",
    "    gpt = pd.concat(dfs, ignore_index=True)\n",
    "    if ENTRY_NUMBER:\n",
    "        gpt = gpt.sample(ENTRY_NUMBER, random_state=random_state)\n",
    "    gpt.to_csv(f'{keyword}.csv', index=False)\n",
    "    return gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165307</th>\n",
       "      <td>The Federal Home and Community-Based Services ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>Snake &amp; Pistol The monkey mask is in a hole to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170212</th>\n",
       "      <td>I am still waiting to hear back on two of the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183081</th>\n",
       "      <td>We have not been able to solve this case, but ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72960</th>\n",
       "      <td>McKenna is getting more benefits than just wit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41502</th>\n",
       "      <td>This section concerns content related to Warcr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131318</th>\n",
       "      <td>The U.S. stationing of an anti-missile defense...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105242</th>\n",
       "      <td>The 2099 panel is one of the highlights of thi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45921</th>\n",
       "      <td>Jackson County officials are interested in own...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86412</th>\n",
       "      <td>Van Gaal added one thing the majority of clubs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  generated\n",
       "165307  The Federal Home and Community-Based Services ...          1\n",
       "1969    Snake & Pistol The monkey mask is in a hole to...          1\n",
       "170212  I am still waiting to hear back on two of the ...          1\n",
       "183081  We have not been able to solve this case, but ...          1\n",
       "72960   McKenna is getting more benefits than just wit...          1\n",
       "...                                                   ...        ...\n",
       "41502   This section concerns content related to Warcr...          1\n",
       "131318  The U.S. stationing of an anti-missile defense...          1\n",
       "105242  The 2099 panel is one of the highlights of thi...          1\n",
       "45921   Jackson County officials are interested in own...          1\n",
       "86412   Van Gaal added one thing the majority of clubs...          1\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "munge_gpt('small-117M-k40')\n",
    "munge_gpt('small-117M')\n",
    "munge_gpt('medium-345M-k40')\n",
    "munge_gpt('medium-345M')\n",
    "munge_gpt('large-762M-k40')\n",
    "munge_gpt('large-762M')\n",
    "munge_gpt('xl-1542M-k40')\n",
    "munge_gpt('xl-1542M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_blog_xml(file):\n",
    "    tree = lxml.html.parse(file)\n",
    "    post_els = tree.findall('.//post')\n",
    "    posts = [p.text.replace('urlLink', '').strip() for p in post_els]\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOG_FILE_NUM = 0 # change this to add blog data\n",
    "def process_human(fake=False):\n",
    "    dfs = []\n",
    "\n",
    "    # files = glob.glob('nyt_data/*.csv')\n",
    "    # for file in files:\n",
    "    #     raw_df = pd.read_csv(file)\n",
    "    #     text_list = [str(t).strip() for t in raw_df['abstract']]\n",
    "    #     generated = [1 if random.random() < 0.5 else 0 for _ in text_list] if fake else 0\n",
    "    #     df = pd.DataFrame({'text': text_list,\n",
    "    #                    'generated': generated})\n",
    "    #     dfs.append(df)\n",
    "\n",
    "    nyt_scraped_files = glob.glob('../nyt_scraper/data/*.txt')\n",
    "    for file in nyt_scraped_files:\n",
    "        lines = open(file).readlines()\n",
    "        text_list = [l.strip() for l in lines]\n",
    "        text_list = text_list[1:] # skip titles\n",
    "        text_list = list(filter(lambda t: t!= \"\" and len(t.split()) >= 10, text_list))\n",
    "        generated = [1 if random.random() < 0.5 else 0 for _ in text_list] if fake else 0\n",
    "        df = pd.DataFrame({'text': text_list,\n",
    "                       'generated': generated})\n",
    "        dfs.append(df)\n",
    "\n",
    "    # blog_files = glob.glob('blogs/*.xml')[:BLOG_FILE_NUM]\n",
    "    # for file in blog_files:\n",
    "    #     posts = parse_blog_xml(file)\n",
    "    #     generated = [1 if random.random() < 0.5 else 0 for _ in posts] if fake else 0\n",
    "    #     df = pd.DataFrame({'text': posts,\n",
    "    #                    'generated': generated})\n",
    "    #     dfs.append(df)\n",
    "\n",
    "    human = pd.concat(dfs, ignore_index=True)\n",
    "    outfile = 'fake.csv' if fake else 'human.csv'\n",
    "    human.to_csv(outfile, index=False)\n",
    "    return human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_human(True)\n",
    "human = process_human(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WASHINGTON — President Trump held what he call...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“I have many issues,” Mr. Trump said in announ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To mark the occasion, several high-ranking adm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The executive order, according to White House,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>According to Polaris, a nonprofit organization...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94264</th>\n",
       "      <td>The program, The Globe wrote, was “a penetrati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94265</th>\n",
       "      <td>Ms. Jarvis began filming in China in August 19...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94266</th>\n",
       "      <td>Ms. Jarvis left NBC in 1976 and founded her ow...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94267</th>\n",
       "      <td>In addition to “Junon and Avos,” which Ms. Jar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94268</th>\n",
       "      <td>Ms. Jarvis’s husband died in 1999. A daughter,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94269 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  generated\n",
       "0      WASHINGTON — President Trump held what he call...          0\n",
       "1      “I have many issues,” Mr. Trump said in announ...          0\n",
       "2      To mark the occasion, several high-ranking adm...          0\n",
       "3      The executive order, according to White House,...          0\n",
       "4      According to Polaris, a nonprofit organization...          0\n",
       "...                                                  ...        ...\n",
       "94264  The program, The Globe wrote, was “a penetrati...          0\n",
       "94265  Ms. Jarvis began filming in China in August 19...          0\n",
       "94266  Ms. Jarvis left NBC in 1976 and founded her ow...          0\n",
       "94267  In addition to “Junon and Avos,” which Ms. Jar...          0\n",
       "94268  Ms. Jarvis’s husband died in 1999. A daughter,...          0\n",
       "\n",
       "[94269 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
