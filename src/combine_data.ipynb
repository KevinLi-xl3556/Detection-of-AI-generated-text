{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032435ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as w\n",
    "random_state = 114514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6bb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "gpt_data = pd.read_csv(\"data_process/gpt.csv\")\n",
    "human_data = pd.read_csv(\"data_process/human.csv\")\n",
    "\n",
    "# Combine the two datasets into one\n",
    "data = pd.concat([gpt_data, human_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d66d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction functions\n",
    "def sentence_length(text):\n",
    "    text = str(text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    numberofsentences = len(sentences)\n",
    "    total_words = 0\n",
    "    for i in sentences:\n",
    "        total_words += len(i.split())\n",
    "    avg_sentence = total_words / numberofsentences\n",
    "    return numberofsentences, avg_sentence\n",
    "\n",
    "def repetitivewords(text):\n",
    "    text = str(text)\n",
    "    token = nltk.word_tokenize(text.lower())\n",
    "    synsets = []\n",
    "    for i in token:\n",
    "        synsets.extend(w.synsets(i))\n",
    "    synonyms = []\n",
    "    for synset in synsets:\n",
    "        synonyms.append([lemma.name() for lemma in synset.lemmas()])\n",
    "    repeat = 0\n",
    "    for index in range(len(synonyms)):\n",
    "        for nextindex in range(index+1, len(synonyms)):\n",
    "            if len(set(synonyms[index]) & set(synonyms[nextindex])) > 0:\n",
    "                repeat += 1\n",
    "    return repeat / len(token)\n",
    "\n",
    "def entropy(text):\n",
    "    text = str(text)\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokennumber = Counter(tokens)\n",
    "    total = len(tokens)\n",
    "    numberofprobs = []\n",
    "    for count in tokennumber.values():\n",
    "        prob = count / total\n",
    "        numberofprobs.append(prob)\n",
    "    entropy = 0.0\n",
    "    for i in numberofprobs:\n",
    "        if i > 0:\n",
    "            entropy -= i * (math.log(i, 2))\n",
    "    return entropy\n",
    "\n",
    "# Extract features\n",
    "data['sent_length'], data['avg_sent_length'] = zip(*data['text'].apply(sentence_length))\n",
    "data['repetitive_words'] = data['text'].apply(repetitivewords)\n",
    "data['text_entropy'] = data['text'].apply(entropy)\n",
    "\n",
    "# save the DataFrame to a CSV file\n",
    "data.to_csv('data_process/processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1cd3de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data_process/processed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dfd77679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generated\n",
       "0    101643\n",
       "1     10000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['generated'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2b05a879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generated\n",
       "1    10000\n",
       "0    10000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_to_drop = data['generated'].value_counts()[0] - data['generated'].value_counts()[1]\n",
    "data = data.drop(data[data['generated'] == 0].sample(n=num_to_drop).index)\n",
    "data['generated'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb30fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6acd5267",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_keep = 6000\n",
    "sampled_data = data.groupby('generated', group_keys=False).apply(lambda x: x.sample(n=rows_to_keep))\n",
    "sampled_data.to_csv('data_process/processed_sample_data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a833564",
   "metadata": {},
   "source": [
    "### Sample gpt3 data with human data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e684a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as w\n",
    "random_state = 114514\n",
    "# Load data\n",
    "rows_to_keep = 5000\n",
    "gpt_data = pd.read_csv(\"data_process/gpt3.csv\").sample(n=rows_to_keep, random_state=random_state)\n",
    "human_data = pd.read_csv(\"data_process/human.csv\").sample(n=rows_to_keep, random_state=random_state)\n",
    "\n",
    "# Combine the two datasets into one\n",
    "data = pd.concat([gpt_data, human_data], ignore_index=True)\n",
    "\n",
    "# Feature extraction functions\n",
    "def sentence_length(text):\n",
    "    text = str(text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    numberofsentences = len(sentences)\n",
    "    total_words = 0\n",
    "    for i in sentences:\n",
    "        total_words += len(i.split())\n",
    "    avg_sentence = total_words / numberofsentences\n",
    "    return numberofsentences, avg_sentence\n",
    "\n",
    "def repetitivewords(text):\n",
    "    text = str(text)\n",
    "    token = nltk.word_tokenize(text.lower())\n",
    "    synsets = []\n",
    "    for i in token:\n",
    "        synsets.extend(w.synsets(i))\n",
    "    synonyms = []\n",
    "    for synset in synsets:\n",
    "        synonyms.append([lemma.name() for lemma in synset.lemmas()])\n",
    "    repeat = 0\n",
    "    for index in range(len(synonyms)):\n",
    "        for nextindex in range(index+1, len(synonyms)):\n",
    "            if len(set(synonyms[index]) & set(synonyms[nextindex])) > 0:\n",
    "                repeat += 1\n",
    "    return repeat / len(token)\n",
    "\n",
    "def entropy(text):\n",
    "    text = str(text)\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokennumber = Counter(tokens)\n",
    "    total = len(tokens)\n",
    "    numberofprobs = []\n",
    "    for count in tokennumber.values():\n",
    "        prob = count / total\n",
    "        numberofprobs.append(prob)\n",
    "    entropy = 0.0\n",
    "    for i in numberofprobs:\n",
    "        if i > 0:\n",
    "            entropy -= i * (math.log(i, 2))\n",
    "    return entropy\n",
    "\n",
    "# Extract features\n",
    "data['sent_length'], data['avg_sent_length'] = zip(*data['text'].apply(sentence_length))\n",
    "data['repetitive_words'] = data['text'].apply(repetitivewords)\n",
    "data['text_entropy'] = data['text'].apply(entropy)\n",
    "\n",
    "# save the DataFrame to a CSV file\n",
    "data.to_csv('data_process/processed_sample_data_gpt3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e9626e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As the weeks dragged on, Ghosn became increasi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Quadrantids meteor shower is known for its...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The case has also led to calls for greater int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The tariffs have not only hurt American consum...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Question 3: What is the origin of the word “li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>The man behind Trump’s favorite unproven treat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Mr. Neumann, who has seven children and many m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>In a lengthy and impassioned dissent, Judge Jo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Institutions will face ethical and sometimes l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>“It wasn’t bad for the ducks before,” he said,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  generated\n",
       "0     As the weeks dragged on, Ghosn became increasi...          1\n",
       "1     The Quadrantids meteor shower is known for its...          1\n",
       "2     The case has also led to calls for greater int...          1\n",
       "3     The tariffs have not only hurt American consum...          1\n",
       "4     Question 3: What is the origin of the word “li...          1\n",
       "...                                                 ...        ...\n",
       "9995  The man behind Trump’s favorite unproven treat...          0\n",
       "9996  Mr. Neumann, who has seven children and many m...          0\n",
       "9997  In a lengthy and impassioned dissent, Judge Jo...          0\n",
       "9998  Institutions will face ethical and sometimes l...          0\n",
       "9999  “It wasn’t bad for the ducks before,” he said,...          0\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338029a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
